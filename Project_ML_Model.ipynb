{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project1_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "ck0giJf-ANUQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCTSkDzJiY34",
        "outputId": "6cd6a804-189a-452b-c461-44fe3973f0f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "504/504 [==============================] - 37s 71ms/step - loss: 0.6807 - accuracy: 0.5487 - val_loss: 0.6664 - val_accuracy: 0.5866\n",
            "Epoch 2/20\n",
            "504/504 [==============================] - 36s 72ms/step - loss: 0.6487 - accuracy: 0.6201 - val_loss: 0.6474 - val_accuracy: 0.6341\n",
            "Epoch 3/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.6248 - accuracy: 0.6533 - val_loss: 0.6398 - val_accuracy: 0.6287\n",
            "Epoch 4/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.6027 - accuracy: 0.6721 - val_loss: 0.6093 - val_accuracy: 0.6729\n",
            "Epoch 5/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.5840 - accuracy: 0.6933 - val_loss: 0.6102 - val_accuracy: 0.6741\n",
            "Epoch 6/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.5703 - accuracy: 0.7041 - val_loss: 0.6048 - val_accuracy: 0.6747\n",
            "Epoch 7/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.5499 - accuracy: 0.7181 - val_loss: 0.5936 - val_accuracy: 0.6902\n",
            "Epoch 8/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.5337 - accuracy: 0.7351 - val_loss: 0.5821 - val_accuracy: 0.6951\n",
            "Epoch 9/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.5151 - accuracy: 0.7417 - val_loss: 0.5684 - val_accuracy: 0.7103\n",
            "Epoch 10/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.4962 - accuracy: 0.7587 - val_loss: 0.5732 - val_accuracy: 0.7016\n",
            "Epoch 11/20\n",
            "504/504 [==============================] - 35s 70ms/step - loss: 0.4808 - accuracy: 0.7688 - val_loss: 0.5736 - val_accuracy: 0.7125\n",
            "Epoch 12/20\n",
            "504/504 [==============================] - 35s 70ms/step - loss: 0.4587 - accuracy: 0.7814 - val_loss: 0.5927 - val_accuracy: 0.6999\n",
            "Epoch 13/20\n",
            "504/504 [==============================] - 35s 70ms/step - loss: 0.4422 - accuracy: 0.7914 - val_loss: 0.5897 - val_accuracy: 0.7045\n",
            "Epoch 14/20\n",
            "504/504 [==============================] - 35s 70ms/step - loss: 0.4192 - accuracy: 0.8056 - val_loss: 0.5909 - val_accuracy: 0.7082\n",
            "Epoch 15/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.3964 - accuracy: 0.8196 - val_loss: 0.6158 - val_accuracy: 0.7118\n",
            "Epoch 16/20\n",
            "504/504 [==============================] - 35s 70ms/step - loss: 0.3799 - accuracy: 0.8269 - val_loss: 0.6161 - val_accuracy: 0.7170\n",
            "Epoch 17/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.3560 - accuracy: 0.8438 - val_loss: 0.6142 - val_accuracy: 0.7193\n",
            "Epoch 18/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.3374 - accuracy: 0.8527 - val_loss: 0.6342 - val_accuracy: 0.7179\n",
            "Epoch 19/20\n",
            "504/504 [==============================] - 35s 70ms/step - loss: 0.3188 - accuracy: 0.8636 - val_loss: 0.6682 - val_accuracy: 0.7100\n",
            "Epoch 20/20\n",
            "504/504 [==============================] - 36s 71ms/step - loss: 0.2890 - accuracy: 0.8801 - val_loss: 0.6914 - val_accuracy: 0.7073\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe5d5895e10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Python program to create Image Classifier using CNN \n",
        "#  ________________________________________________________________\n",
        "import cv2 \n",
        "import os \n",
        "import numpy as np \n",
        "from random import shuffle \n",
        "from tqdm import tqdm \n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "'''Setting up the env'''\n",
        "\n",
        "TRAIN_DIR = '/content/drive/MyDrive/train'\n",
        "TEST_DIR = '/content/drive/MyDrive/test'\n",
        "IMG_SIZE = 30\n",
        "\n",
        "CATEGORIES = ['cats', 'dogs']\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(TRAIN_DIR, category)\n",
        "    for img in os.listdir(path):\n",
        "        img_path = os.path.join(path, img)\n",
        "        label = CATEGORIES.index(category)\n",
        "        arr = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        new_arr = cv2.resize(arr, (IMG_SIZE, IMG_SIZE))\n",
        "        train_data.append([new_arr, label])\n",
        "\n",
        "for img in os.listdir(TEST_DIR):\n",
        "  img_path = os.path.join(TEST_DIR, img)\n",
        "  img_num = img.split('.')[0] \n",
        "  arr = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "  new_arr = cv2.resize(arr, (IMG_SIZE, IMG_SIZE))\n",
        "  test_data.append([new_arr, img_num])\n",
        "\n",
        "random.shuffle(train_data)\n",
        "random.shuffle(test_data)\n",
        "\n",
        "train_X = []\n",
        "train_Y = []\n",
        "for features, label in train_data:\n",
        "    train_X.append(features)\n",
        "    train_Y.append(label)\n",
        "\n",
        "train_X = np.array(train_X) / 255\n",
        "train_Y = np.array(train_Y)\n",
        "train_X = train_X.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "#from keras.losses import SparseCategoricalCrossentropy\n",
        "import tensorflow as tf\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, (7,7), activation = 'relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (5,5), activation = 'relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(8, input_shape = train_X.shape[1:], activation = 'sigmoid'))\n",
        "model.add(Dense(2, activation = 'sigmoid'))\n",
        "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_X, train_Y, epochs = 20, validation_split=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weights Extraction"
      ],
      "metadata": {
        "id": "6roCIwYHAU1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hluE2wTj0-z",
        "outputId": "aec93657-1ffc-4419-f7c9-30b2451845ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 24, 24, 64)        3200      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 64)          102464    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 4, 4, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 2, 2, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 1, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8)                 520       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 18        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 143,130\n",
            "Trainable params: 143,130\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 0-1\n",
        "WeightsLayer01 = np.transpose(model.layers[7].get_weights()[0])\n",
        "BiasesLayer01 = model.layers[7].get_weights()[1]\n",
        "#print(WeightsLayer01)"
      ],
      "metadata": {
        "id": "4-GBJPeuAddl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 1-2\n",
        "WeightsLayer12 = np.transpose(model.layers[8].get_weights()[0])\n",
        "BiasesLayer12 = model.layers[8].get_weights()[1]\n",
        "#print(WeightsLayer12)"
      ],
      "metadata": {
        "id": "j1hZPAiUAfTQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Weights Conversion from decimal to floating point representation to hexadecimal and saving to .txt\n"
      ],
      "metadata": {
        "id": "wD9cEgUxAiwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DecimalToFloatPt32_hex_txt(matrix, name):\n",
        "  #### Decimal to single precision floating point representation\n",
        "  import struct\n",
        "  def floatToBinary32(value):\n",
        "      return ''.join(f'{c:0>8b}' for c in struct.pack('!f', value))\n",
        "\n",
        "  matrix_FloatPt32_binary = []\n",
        "  for row in matrix:\n",
        "    temp=[]\n",
        "    for values in row:\n",
        "    #print(values)\n",
        "    #break\n",
        "      temp.append(floatToBinary32(values))\n",
        "    matrix_FloatPt32_binary.append(temp)\n",
        "\n",
        "\n",
        "  #### single precision floating point representation to Hexadecimal\n",
        "  matrix_FloatPt32_hex = []\n",
        "  for row in matrix_FloatPt32_binary:\n",
        "    temp = []\n",
        "    for values in row:\n",
        "      temp.append(\"{0:0>4X}\".format(int(values, 2)))\n",
        "    matrix_FloatPt32_hex.append(temp)\n",
        "\n",
        "  #### writes each weight in each new line\n",
        "  file = open(name, \"w+\")\n",
        "  for row in matrix_FloatPt32_hex:\n",
        "    for values in row:\n",
        "      file.write(values)\n",
        "      file.write(\"\\n\")\n",
        "  file.close()\n",
        "\n",
        "# Weights of Layer 0-1\n",
        "DecimalToFloatPt32_hex_txt(WeightsLayer01, \"WeightsLayer01_FloatPt32_hex.txt\")"
      ],
      "metadata": {
        "id": "UFEj8_h6Ak9c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bias of Layer 0-1\n",
        "#### Decimal to single precision floating point representation\n",
        "import struct\n",
        "def floatToBinary32(value):\n",
        "    return ''.join(f'{c:0>8b}' for c in struct.pack('!f', value))\n",
        "\n",
        "BiasesLayer01_FloatPt32_binary = []\n",
        "for row in BiasesLayer01:\n",
        "  BiasesLayer01_FloatPt32_binary.append(floatToBinary32(row))\n",
        "\n",
        "\n",
        "#### single precision floating point representation to Hexadecimal\n",
        "BiasesLayer01_FloatPt32_hex = []\n",
        "for row in BiasesLayer01_FloatPt32_binary:\n",
        "  BiasesLayer01_FloatPt32_hex.append(\"{0:0>4X}\".format(int(row, 2)))\n",
        "\n",
        "#### writes each weight in each new line\n",
        "file = open(\"BiasesLayer01_FloatPt32_hex.txt\", \"w+\")\n",
        "for row in BiasesLayer01_FloatPt32_hex:\n",
        "  file.write(row)\n",
        "  file.write(\"\\n\")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "y0IDtOZvAnMZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights of Layer 1-2\n",
        "DecimalToFloatPt32_hex_txt(WeightsLayer12, \"WeightsLayer12_FloatPt32_hex.txt\")"
      ],
      "metadata": {
        "id": "pjeiM7zGApg1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bias of Layer 1-2\n",
        "#### Decimal to single precision floating point representation\n",
        "import struct\n",
        "def floatToBinary32(value):\n",
        "    return ''.join(f'{c:0>8b}' for c in struct.pack('!f', value))\n",
        "\n",
        "BiasesLayer12_FloatPt32_binary = []\n",
        "for row in BiasesLayer12:\n",
        "  BiasesLayer12_FloatPt32_binary.append(floatToBinary32(row))\n",
        "\n",
        "\n",
        "#### single precision floating point representation to Hexadecimal\n",
        "BiasesLayer12_FloatPt32_hex = []\n",
        "for row in BiasesLayer12_FloatPt32_binary:\n",
        "  BiasesLayer12_FloatPt32_hex.append(\"{0:0>4X}\".format(int(row, 2)))\n",
        "\n",
        "#### writes each weight in each new line\n",
        "file = open(\"BiasesLayer12_FloatPt32_hex.txt\", \"w+\")\n",
        "for row in BiasesLayer12_FloatPt32_hex:\n",
        "  file.write(row)\n",
        "  file.write(\"\\n\")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "4-R5xy0ZApZW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#From .txt file to .mif format"
      ],
      "metadata": {
        "id": "kxQpm49pAtb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This script helps to convert hex instructions to Quartus's .mif files.\n",
        "To run it, use \"python hex2mif.py input.txt output.mif\"\n",
        "For input file, assuming you are using Mars as assember,\n",
        "you need to export the binary instruction as hexadecimal text file.\n",
        "But for any kind of input files, it should work fine as long as\n",
        "it uses the following format:\n",
        "DEADBEEF\n",
        "BAADF00D\n",
        "'''\n",
        "\n",
        "import sys\n",
        "\n",
        "def read(f_in):\n",
        "    with open(f_in) as f:\n",
        "        return [int(i, 16) for i in f if len(i) != 0]\n",
        "\n",
        "\n",
        "def write(f_out, data, width=32, depth=65536):\n",
        "    if len(data) > depth:\n",
        "        print('Data larger than memory size, abort.')\n",
        "    else:\n",
        "        buf = 'WIDTH={:d};\\nDEPTH={:d};\\nADDRESS_RADIX=HEX;\\nDATA_RADIX=HEX;\\nCONTENT BEGIN\\n'.format(width, depth)\n",
        "        l_index = str(len('{:x}'.format(depth)))\n",
        "        l_data = str(int(width / 4))\n",
        "        s = '\\t{:0' + l_index + 'X} : {:0' + l_data + 'X};\\n'\n",
        "        for i, j in enumerate(data):\n",
        "            buf += s.format(i, j)\n",
        "        if len(buf) == depth - 1:\n",
        "            buf += s.format(depth - 1, 0)\n",
        "        elif len(buf) < depth - 1:\n",
        "            buf += ('\\t[{:0' + l_index + 'X}..{:0' + l_index + 'X}] : {:0' + l_data + 'X};\\n').format(len(data), depth - 1, 0)\n",
        "        buf += 'END;\\n'\n",
        "        with open(f_out, 'w') as f:\n",
        "            f.write(buf)\n",
        "\n",
        "\n",
        "def convert(f_in, f_out):\n",
        "    write(f_out, read(f_in))\n",
        "\n"
      ],
      "metadata": {
        "id": "s8qf1Oa_ApQ9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Weights\n",
        "convert(\"WeightsLayer01_FloatPt32_hex.txt\", \"WeightsLayer01_FloatPt32_hex.mif\")\n",
        "convert(\"WeightsLayer12_FloatPt32_hex.txt\", \"WeightsLayer12_FloatPt32_hex.mif\")\n",
        "\n",
        "#Bias\n",
        "convert(\"BiasesLayer01_FloatPt32_hex.txt\", \"BiasesLayer01_FloatPt32_hex.mif\")\n",
        "convert(\"BiasesLayer12_FloatPt32_hex.txt\", \"BiasesLayer12_FloatPt32_hex.mif\")"
      ],
      "metadata": {
        "id": "DlZa5SXvAyrH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input Image Conversion from decimal to floating point representation to hexadecimal and saving to .txt"
      ],
      "metadata": {
        "id": "fsUuHhLSA0fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(10):\n",
        "  from keras import backend as K\n",
        "  # with a Sequential model\n",
        "  get_3rd_layer_output = K.function([model.layers[0].input], [model.layers[6].output])\n",
        "  layer_output = get_3rd_layer_output([train_data[i][0].reshape(-1,IMG_SIZE, IMG_SIZE, 1) ])[0]\n",
        "  #print(layer_output)\n",
        "\n",
        "\n",
        "  #### Decimal to single precision floating point representation\n",
        "  import struct\n",
        "  def floatToBinary32(value):\n",
        "      return ''.join(f'{c:0>8b}' for c in struct.pack('!f', value))\n",
        "\n",
        "  ip_FloatPt32_binary = []\n",
        "  for row in layer_output[0]:\n",
        "    ip_FloatPt32_binary.append(floatToBinary32(row))\n",
        "\n",
        "\n",
        "  #### single precision floating point representation to Hexadecimal\n",
        "  ip_FloatPt32_hex = []\n",
        "  for row in ip_FloatPt32_binary:\n",
        "    ip_FloatPt32_hex.append(\"{0:0>4X}\".format(int(row, 2)))\n",
        "\n",
        "  #### writes each weight in each new line\n",
        "  file = open(\"ip\"+str(i+1)+\".txt\", \"w+\")\n",
        "  for row in ip_FloatPt32_hex:\n",
        "    file.write(row)\n",
        "    file.write(\"\\n\")\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "iFN29VtJrY9-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#From .txt file to .mif format"
      ],
      "metadata": {
        "id": "MvSACLRBA6q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This script helps to convert hex instructions to Quartus's .mif files.\n",
        "To run it, use \"python hex2mif.py input.txt output.mif\"\n",
        "For input file, assuming you are using Mars as assember,\n",
        "you need to export the binary instruction as hexadecimal text file.\n",
        "But for any kind of input files, it should work fine as long as\n",
        "it uses the following format:\n",
        "DEADBEEF\n",
        "BAADF00D\n",
        "'''\n",
        "\n",
        "import sys\n",
        "\n",
        "def read(f_in):\n",
        "    with open(f_in) as f:\n",
        "        return [int(i, 16) for i in f if len(i) != 0]\n",
        "\n",
        "\n",
        "def write(f_out, data, width=32, depth=65536):\n",
        "    if len(data) > depth:\n",
        "        print('Data larger than memory size, abort.')\n",
        "    else:\n",
        "        buf = 'WIDTH={:d};\\nDEPTH={:d};\\nADDRESS_RADIX=HEX;\\nDATA_RADIX=HEX;\\nCONTENT BEGIN\\n'.format(width, depth)\n",
        "        l_index = str(len('{:x}'.format(depth)))\n",
        "        l_data = str(int(width / 4))\n",
        "        s = '\\t{:0' + l_index + 'X} : {:0' + l_data + 'X};\\n'\n",
        "        for i, j in enumerate(data):\n",
        "            buf += s.format(i, j)\n",
        "        if len(buf) == depth - 1:\n",
        "            buf += s.format(depth - 1, 0)\n",
        "        elif len(buf) < depth - 1:\n",
        "            buf += ('\\t[{:0' + l_index + 'X}..{:0' + l_index + 'X}] : {:0' + l_data + 'X};\\n').format(len(data), depth - 1, 0)\n",
        "        buf += 'END;\\n'\n",
        "        with open(f_out, 'w') as f:\n",
        "            f.write(buf)\n",
        "\n",
        "\n",
        "def convert(f_in, f_out):\n",
        "    write(f_out, read(f_in))\n",
        "\n",
        "for i in range(10):\n",
        "  convert(\"ip\"+str(i+1)+\".txt\", \"ip\"+str(i+1)+\".mif\")"
      ],
      "metadata": {
        "id": "8xadYJnvA8pZ"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}